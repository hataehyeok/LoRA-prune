`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.21s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.43s/it]
Pruning the model...

Traceback (most recent call last):
  File "/root/lora-prune/gemma/gemma2b-glue.py", line 313, in <module>
    prune_and_knowledge_distillation(test_data)
  File "/root/lora-prune/gemma/gemma2b-glue.py", line 257, in prune_and_knowledge_distillation
    threshold = torch.quantile(weight_tensor.abs(), sparsity)
RuntimeError: quantile() input tensor is too large
