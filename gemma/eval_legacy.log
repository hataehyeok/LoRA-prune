`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:04<00:04,  4.09s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.82s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.16s/it]
Accuracy on test cases: 92.31%
