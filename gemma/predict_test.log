`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.
Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use
`config.hidden_activation` if you want to override this behaviour.
See https://github.com/huggingface/transformers/pull/29402 for more details.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.49s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.15s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.35s/it]
/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length. Will not be supported from version '0.13.0'.

Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.
  warnings.warn(message, FutureWarning)
/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.
  warnings.warn(
max_steps is given, it will override any value given in num_train_epochs
  0%|          | 0/100 [00:00<?, ?it/s]  1%|          | 1/100 [00:01<03:08,  1.91s/it]  2%|▏         | 2/100 [00:03<02:54,  1.78s/it]  3%|▎         | 3/100 [00:05<02:45,  1.71s/it]  4%|▍         | 4/100 [00:06<02:46,  1.73s/it]  5%|▌         | 5/100 [00:08<02:44,  1.73s/it]  6%|▌         | 6/100 [00:10<02:33,  1.63s/it]  7%|▋         | 7/100 [00:11<02:35,  1.67s/it]  8%|▊         | 8/100 [00:13<02:30,  1.64s/it]  9%|▉         | 9/100 [00:15<02:27,  1.62s/it] 10%|█         | 10/100 [00:16<02:27,  1.64s/it]                                                 10%|█         | 10/100 [00:16<02:27,  1.64s/it] 11%|█         | 11/100 [00:18<02:27,  1.65s/it] 12%|█▏        | 12/100 [00:19<02:21,  1.61s/it] 13%|█▎        | 13/100 [00:21<02:22,  1.64s/it] 14%|█▍        | 14/100 [00:23<02:26,  1.70s/it] 15%|█▌        | 15/100 [00:25<02:25,  1.71s/it] 16%|█▌        | 16/100 [00:27<02:36,  1.86s/it] 17%|█▋        | 17/100 [00:29<02:28,  1.79s/it] 18%|█▊        | 18/100 [00:30<02:21,  1.72s/it] 19%|█▉        | 19/100 [00:32<02:19,  1.72s/it] 20%|██        | 20/100 [00:34<02:18,  1.73s/it]                                                 20%|██        | 20/100 [00:34<02:18,  1.73s/it] 21%|██        | 21/100 [00:35<02:19,  1.77s/it] 22%|██▏       | 22/100 [00:37<02:15,  1.74s/it] 23%|██▎       | 23/100 [00:39<02:26,  1.90s/it] 24%|██▍       | 24/100 [00:41<02:19,  1.84s/it] 25%|██▌       | 25/100 [00:43<02:12,  1.76s/it] 26%|██▌       | 26/100 [00:44<02:05,  1.69s/it] 27%|██▋       | 27/100 [00:46<02:12,  1.82s/it] 28%|██▊       | 28/100 [00:48<02:08,  1.78s/it] 29%|██▉       | 29/100 [00:50<02:11,  1.86s/it] 30%|███       | 30/100 [00:52<02:07,  1.82s/it]                                                 30%|███       | 30/100 [00:52<02:07,  1.82s/it] 31%|███       | 31/100 [00:54<02:04,  1.80s/it] 32%|███▏      | 32/100 [00:55<01:58,  1.75s/it] 33%|███▎      | 33/100 [00:57<01:56,  1.74s/it] 34%|███▍      | 34/100 [00:59<01:53,  1.72s/it] 35%|███▌      | 35/100 [01:00<01:52,  1.73s/it] 36%|███▌      | 36/100 [01:02<01:51,  1.75s/it] 37%|███▋      | 37/100 [01:04<01:50,  1.76s/it] 38%|███▊      | 38/100 [01:06<01:52,  1.82s/it] 39%|███▉      | 39/100 [01:08<01:52,  1.84s/it] 40%|████      | 40/100 [01:10<01:58,  1.97s/it]                                                 40%|████      | 40/100 [01:10<01:58,  1.97s/it] 41%|████      | 41/100 [01:12<01:56,  1.97s/it] 42%|████▏     | 42/100 [01:14<01:49,  1.88s/it] 43%|████▎     | 43/100 [01:15<01:43,  1.81s/it] 44%|████▍     | 44/100 [01:17<01:39,  1.77s/it] 45%|████▌     | 45/100 [01:19<01:39,  1.81s/it] 46%|████▌     | 46/100 [01:20<01:33,  1.74s/it] 47%|████▋     | 47/100 [01:22<01:29,  1.68s/it] 48%|████▊     | 48/100 [01:24<01:28,  1.71s/it] 49%|████▉     | 49/100 [01:26<01:30,  1.78s/it] 50%|█████     | 50/100 [01:27<01:26,  1.74s/it]                                                 50%|█████     | 50/100 [01:27<01:26,  1.74s/it] 51%|█████     | 51/100 [01:29<01:23,  1.70s/it] 52%|█████▏    | 52/100 [01:31<01:22,  1.72s/it] 53%|█████▎    | 53/100 [01:33<01:25,  1.82s/it] 54%|█████▍    | 54/100 [01:34<01:22,  1.79s/it] 55%|█████▌    | 55/100 [01:36<01:22,  1.83s/it] 56%|█████▌    | 56/100 [01:38<01:22,  1.89s/it] 57%|█████▋    | 57/100 [01:40<01:22,  1.91s/it] 58%|█████▊    | 58/100 [01:42<01:20,  1.92s/it] 59%|█████▉    | 59/100 [01:44<01:15,  1.84s/it] 60%|██████    | 60/100 [01:46<01:14,  1.85s/it]                                                 60%|██████    | 60/100 [01:46<01:14,  1.85s/it] 61%|██████    | 61/100 [01:47<01:09,  1.78s/it] 62%|██████▏   | 62/100 [01:49<01:07,  1.77s/it] 63%|██████▎   | 63/100 [01:51<01:05,  1.78s/it] 64%|██████▍   | 64/100 [01:53<01:03,  1.77s/it] 65%|██████▌   | 65/100 [01:55<01:04,  1.83s/it] 66%|██████▌   | 66/100 [01:56<01:00,  1.78s/it] 67%|██████▋   | 67/100 [01:59<01:01,  1.88s/it] 68%|██████▊   | 68/100 [02:00<00:59,  1.86s/it] 69%|██████▉   | 69/100 [02:02<00:58,  1.89s/it] 70%|███████   | 70/100 [02:04<00:55,  1.84s/it]                                                 70%|███████   | 70/100 [02:04<00:55,  1.84s/it] 71%|███████   | 71/100 [02:06<00:53,  1.84s/it] 72%|███████▏  | 72/100 [02:08<00:52,  1.89s/it] 73%|███████▎  | 73/100 [02:09<00:48,  1.79s/it] 74%|███████▍  | 74/100 [02:11<00:45,  1.74s/it] 75%|███████▌  | 75/100 [02:13<00:44,  1.78s/it] 76%|███████▌  | 76/100 [02:15<00:45,  1.91s/it] 77%|███████▋  | 77/100 [02:17<00:41,  1.79s/it] 78%|███████▊  | 78/100 [02:19<00:41,  1.89s/it] 79%|███████▉  | 79/100 [02:21<00:38,  1.85s/it] 80%|████████  | 80/100 [02:22<00:36,  1.83s/it]                                                 80%|████████  | 80/100 [02:22<00:36,  1.83s/it] 81%|████████  | 81/100 [02:24<00:35,  1.87s/it] 82%|████████▏ | 82/100 [02:27<00:38,  2.12s/it] 83%|████████▎ | 83/100 [02:29<00:34,  2.05s/it] 84%|████████▍ | 84/100 [02:31<00:35,  2.22s/it] 85%|████████▌ | 85/100 [02:33<00:30,  2.02s/it] 86%|████████▌ | 86/100 [02:35<00:28,  2.06s/it] 87%|████████▋ | 87/100 [02:37<00:26,  2.04s/it] 88%|████████▊ | 88/100 [02:39<00:25,  2.10s/it] 89%|████████▉ | 89/100 [02:41<00:21,  1.99s/it] 90%|█████████ | 90/100 [02:43<00:20,  2.03s/it]                                                 90%|█████████ | 90/100 [02:43<00:20,  2.03s/it] 91%|█████████ | 91/100 [02:45<00:18,  2.01s/it] 92%|█████████▏| 92/100 [02:47<00:15,  2.00s/it] 93%|█████████▎| 93/100 [02:49<00:13,  1.95s/it] 94%|█████████▍| 94/100 [02:51<00:11,  1.94s/it] 95%|█████████▌| 95/100 [02:53<00:09,  1.94s/it] 96%|█████████▌| 96/100 [02:55<00:07,  1.84s/it] 97%|█████████▋| 97/100 [02:57<00:06,  2.01s/it] 98%|█████████▊| 98/100 [02:59<00:03,  1.99s/it] 99%|█████████▉| 99/100 [03:01<00:01,  1.89s/it]100%|██████████| 100/100 [03:03<00:00,  1.91s/it]                                                 100%|██████████| 100/100 [03:03<00:00,  1.91s/it]                                                 100%|██████████| 100/100 [03:04<00:00,  1.91s/it]100%|██████████| 100/100 [03:04<00:00,  1.84s/it]
{'loss': 12.9376, 'grad_norm': 10.122769355773926, 'learning_rate': 2e-05, 'epoch': 0.0}
{'loss': 11.7992, 'grad_norm': 16.505157470703125, 'learning_rate': 3.6e-05, 'epoch': 0.0}
{'loss': 9.4495, 'grad_norm': 31.2147216796875, 'learning_rate': 5.4000000000000005e-05, 'epoch': 0.01}
{'loss': 6.8725, 'grad_norm': 10.858166694641113, 'learning_rate': 7.4e-05, 'epoch': 0.01}
{'loss': 5.1522, 'grad_norm': 5.170043468475342, 'learning_rate': 9.4e-05, 'epoch': 0.01}
{'loss': 4.0214, 'grad_norm': 8.594535827636719, 'learning_rate': 0.00011200000000000001, 'epoch': 0.01}
{'loss': 3.2135, 'grad_norm': 9.9931001663208, 'learning_rate': 0.000132, 'epoch': 0.02}
{'loss': 2.7253, 'grad_norm': 6.498246669769287, 'learning_rate': 0.000152, 'epoch': 0.02}
{'loss': 2.4431, 'grad_norm': 5.776248931884766, 'learning_rate': 0.000172, 'epoch': 0.02}
{'loss': 2.1739, 'grad_norm': 3.6940300464630127, 'learning_rate': 0.000192, 'epoch': 0.02}
{'train_runtime': 184.4416, 'train_samples_per_second': 8.675, 'train_steps_per_second': 0.542, 'train_loss': 6.078827724456787, 'epoch': 0.02}
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:01<00:01,  1.29s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.62it/s]Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.39it/s]
Some parameters are on the meta device because they were offloaded to the cpu.
Traceback (most recent call last):
  File "/home/hth021002/lora-prune/gemma/gemma2b-glue.py", line 259, in <module>
    final_model.save_pretrained("gemma-2b-it-sst2-pruned")
  File "/home/hth021002/lora-prune/gemma/gemma2b-glue.py", line 120, in fine_tuning
    predictions = trainer.predict(test_data)
  File "/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/transformers/trainer.py", line 4053, in predict
    output = eval_loop(
  File "/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/transformers/trainer.py", line 4159, in evaluation_loop
    for step, inputs in enumerate(dataloader):
  File "/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/accelerate/data_loader.py", line 552, in __iter__
    current_batch = next(dataloader_iter)
  File "/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 631, in __next__
    data = self._next_data()
  File "/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/data/dataloader.py", line 675, in _next_data
    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration
  File "/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py", line 54, in fetch
    return self.collate_fn(data)
  File "/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/transformers/data/data_collator.py", line 45, in __call__
    return self.torch_call(features)
  File "/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/transformers/data/data_collator.py", line 807, in torch_call
    batch = pad_without_fast_tokenizer_warning(
  File "/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/transformers/data/data_collator.py", line 66, in pad_without_fast_tokenizer_warning
    padded = tokenizer.pad(*pad_args, **pad_kwargs)
  File "/home/hth021002/miniconda3/envs/test/lib/python3.10/site-packages/transformers/tokenization_utils_base.py", line 3458, in pad
    raise ValueError(
ValueError: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided ['label']
